{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwoJ8YmFb3WDnD/rmrAF1d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsthomp5/MAT422/blob/main/MAT_422_HW_1_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gYont0hyzNmq"
      },
      "outputs": [],
      "source": [
        "# Name: Nicholas Thompson\n",
        "# ID: 1223100502\n",
        "# Class: MAT 422\n",
        "# Assignment: HW 1.4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from numpy import linalg\n",
        "from numpy import random"
      ],
      "metadata": {
        "id": "y34k0gIU3SSQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Singular Value Decomposition (SVD) factors a matrix into three others using eigenvalues and eigenvectors. Applications range from various matrix operations to making image-based representations of data easier to work with. Each of the U, S, and V matrices organizes useful information to be used for various purposes, and a goal of machine learning is to use as easy-to-handle data as possible."
      ],
      "metadata": {
        "id": "0BmMVX-xAnoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.4.1 SVD\n",
        "# Example 1: SVD function\n",
        "\n",
        "A = np.array([[3,4,5,6],[1,4,9,8],[4,5,3,4]]) # Example 3x4 matrix A\n",
        "U, S, V = np.linalg.svd(A) # using SVD function #S can be derived from U or W, diagonal matrix of eigenvalues\n",
        "S = np.diag(S)\n",
        "print(\"Matrix U is \" + str(U))\n",
        "print(\"Matrix S is \" + str(S))\n",
        "print(\"Matrix V is \" + str(V))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eleTjXFYzTgd",
        "outputId": "8ae9a3d3-826f-40b8-abb5-149d1ae9531b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix U is [[ 0.5370015   0.21028875 -0.81695044]\n",
            " [ 0.72527578 -0.60967024  0.31980813]\n",
            " [ 0.43081832  0.76425182  0.47991117]]\n",
            "Matrix S is [[17.15308852  0.          0.        ]\n",
            " [ 0.          4.39592041  0.        ]\n",
            " [ 0.          0.          0.66890802]]\n",
            "Matrix V is [[ 0.23666604  0.41993608  0.61242292  0.62656288]\n",
            " [ 0.7002409   0.50586291 -0.48745946 -0.12707741]\n",
            " [-0.31603525  0.61441422  0.3487093  -0.63326039]\n",
            " [-0.59478869  0.43617837 -0.51548353  0.43617837]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: SVD process breakdown\n",
        "\n",
        "A_t=np.transpose(A) #going through SVD process manually\n",
        "W = A @ A_t\n",
        "\n",
        "\n",
        "U = np.linalg.eig(W)[1] #U matrix comprised of eigenvectors of W\n",
        "print(U)\n",
        "\n",
        "Y = A_t @ A #Obtaining V is reverse of obtaining U\n",
        "\n",
        "\n",
        "V = np.linalg.eig(Y)[1]\n",
        "print(V)\n",
        "\n",
        "S = np.diag(np.sqrt(np.linalg.eig(W)[0])) #S can be derived from U or W, diagonal matrix of eigenvalues\n",
        "print(S)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ido6BX26374r",
        "outputId": "7dbffd53-b118-4233-b876-f825a5004386"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.5370015   0.81695044  0.21028875]\n",
            " [ 0.72527578 -0.31980813 -0.60967024]\n",
            " [ 0.43081832 -0.47991117  0.76425182]]\n",
            "[[ 0.23666604  0.7002409   0.59478869  0.31603525]\n",
            " [ 0.41993608  0.50586291 -0.43617837 -0.61441422]\n",
            " [ 0.61242292 -0.48745946  0.51548353 -0.3487093 ]\n",
            " [ 0.62656288 -0.12707741 -0.43617837  0.63326039]]\n",
            "[[17.15308852  0.          0.        ]\n",
            " [ 0.          0.66890802  0.        ]\n",
            " [ 0.          0.          4.39592041]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Low-rank matrix approximation is like other linear algebra techniques relevant to machine learning, as it reduces the dimensions of a matrix to be less computationally costly when used for machine learning purposes. In this case, low-rank approximations decrease the number of features necessary for a model by reducing a matrix to an approximation of itself with a lower rank, making it less computationally costly to pull data from. This is done using SVD, adding another application to a long list of them."
      ],
      "metadata": {
        "id": "VRT1jr3_H1ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.4.2 Low-rank matrix approximations\n",
        "# Example 1: Using SVD\n",
        "\n",
        "A = np.array([[3,4,5,6],[1,4,9,8],[4,5,3,4]]) # Example 3x4 matrix A\n",
        "U, S, V = np.linalg.svd(A) # using SVD function\n",
        "print(\"Matrix U is \" + str(U))\n",
        "print(\"Matrix S is \" + str(S))\n",
        "print(\"Matrix V is \" + str(V))\n",
        "print(S)\n",
        "print(V)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJB8LQBnr81Q",
        "outputId": "6d4632e9-b2ea-4742-ceea-7446f526206a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix U is [[ 0.5370015   0.21028875 -0.81695044]\n",
            " [ 0.72527578 -0.60967024  0.31980813]\n",
            " [ 0.43081832  0.76425182  0.47991117]]\n",
            "Matrix S is [17.15308852  4.39592041  0.66890802]\n",
            "Matrix V is [[ 0.23666604  0.41993608  0.61242292  0.62656288]\n",
            " [ 0.7002409   0.50586291 -0.48745946 -0.12707741]\n",
            " [-0.31603525  0.61441422  0.3487093  -0.63326039]\n",
            " [-0.59478869  0.43617837 -0.51548353  0.43617837]]\n",
            "[17.15308852  4.39592041  0.66890802]\n",
            "[[ 0.23666604  0.41993608  0.61242292  0.62656288]\n",
            " [ 0.7002409   0.50586291 -0.48745946 -0.12707741]\n",
            " [-0.31603525  0.61441422  0.3487093  -0.63326039]\n",
            " [-0.59478869  0.43617837 -0.51548353  0.43617837]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r = 1 # rank\n",
        "U = U[:, :S.shape[0]] # matching dimensions of U with S\n",
        "S = np.diag(S) # S as a diagonal matrix of eigenvalues\n",
        "\n",
        "\n",
        "R = U[:, :r] @ S[:r, :r] @ V[:r, :] #pinching U, S, V to the lowest rank of the three and multiplying\n",
        "print(R) # low-rank approximation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8_szdLZE6sT",
        "outputId": "e20b9efe-5190-427b-b5f9-aa599ce9d678"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.17998638 3.86812963 5.64117099 5.77141751]\n",
            " [2.94429589 5.22430705 7.61898183 7.79489315]\n",
            " [1.74893007 3.10327087 4.52572259 4.63021501]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis is used to reduce the dimensions of data by choosing a subset of features to analyze. The features with the most variability are chosen first, as they represent the largest differences between the individual data points, so they are chosen as principal features first because they are more likely to be predictive of outputs. Eigenvalues and eigenvectors are used again, as they can be applied to many areas of machine learning, as well as other math and data science applications like differential equations and data structures."
      ],
      "metadata": {
        "id": "hKOqMwZNUyg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.4.3 Principal Component Analysis\n",
        "# Example 1: Baseball Stats\n",
        "\n",
        "x = np.zeros([40,5]) #dataset generation\n",
        "\n",
        "x[:,0] = np.random.randint(60,100,40)/300 # batting average\n",
        "x[:,1] = np.random.randint(300,400,40)/10 # hard-hit %\n",
        "x[:,2] = np.random.randint(0,25,40) # SEAGER\n",
        "x[:,3] = np.random.randint(50,350,40)/100 # ISO\n",
        "x[:,4] = np.random.randint(5,20,40) # pulled flyball %\n",
        "\n",
        "x_centered = np.zeros([40,5])\n",
        "\n",
        "for i in range(5):\n",
        "    x_centered[:,i] = x[:,i] - np.mean(x[:,i]) # centering the data around the mean\n",
        "\n",
        "\n",
        "cov_matrix = np.cov(x_centered, rowvar = False) #covariance matrix\n",
        "\n",
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix) # eigenvalues and eigenvectors of covariance matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "ktPPIjDSGo1l"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, since each eigenvalue represents the variability of a principal component, sorting the eigenvalues will sort the principal components by their variability."
      ],
      "metadata": {
        "id": "bc2MKw5FQ8S5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_comp = np.argsort(eigenvalues)[::-1] # sorts eigenvalues for component variability\n",
        "\n",
        "sorted_eigenvalues = eigenvalues[sorted_comp] # new array\n",
        "\n",
        "sorted_eigenvectors = eigenvectors[sorted_comp] # new array of eigenvectors by eigenvalue sort\n",
        "\n"
      ],
      "metadata": {
        "id": "ZKlMsycoRE0k"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comp_num = 1 # choose number of components"
      ],
      "metadata": {
        "id": "IFXpBWxcRh29"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comp_subset = sorted_eigenvectors[:,0:comp_num] # reduces eigenvector array to chosen number of components to analyze\n",
        "\n",
        "x_reduced = np.dot(comp_subset.transpose(),x_centered.transpose()).transpose() # dot product scales data based on principal components outputs lower-dimensional data\n",
        "\n",
        "print(x_reduced)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBy4CS7tRlHz",
        "outputId": "3ae3c95d-93f8-4cd9-83ba-1432c7ea96a3"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ -8.00180045]\n",
            " [-10.93506398]\n",
            " [  6.04996419]\n",
            " [ -0.20839737]\n",
            " [ -3.58296331]\n",
            " [  6.12628514]\n",
            " [ -8.74690002]\n",
            " [ -0.11915512]\n",
            " [  1.86774561]\n",
            " [ -2.76024026]\n",
            " [  8.7920319 ]\n",
            " [-11.76365844]\n",
            " [ -5.11653793]\n",
            " [  3.87049983]\n",
            " [ -0.2493645 ]\n",
            " [  8.26554559]\n",
            " [  6.97983602]\n",
            " [  1.11525187]\n",
            " [  0.79391242]\n",
            " [ 10.3139042 ]\n",
            " [ 10.07163027]\n",
            " [ 11.12593081]\n",
            " [ -4.84187254]\n",
            " [ -9.06744157]\n",
            " [  6.27157946]\n",
            " [ 11.81321405]\n",
            " [  2.01136671]\n",
            " [  4.03173355]\n",
            " [-12.14924791]\n",
            " [  5.39759902]\n",
            " [ -7.07381111]\n",
            " [ -3.9477822 ]\n",
            " [  7.80649459]\n",
            " [ -8.660521  ]\n",
            " [-10.15467857]\n",
            " [ 12.28625143]\n",
            " [ -7.97126375]\n",
            " [ -3.97442525]\n",
            " [ -5.95839941]\n",
            " [  0.29274802]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting output shows the data rescaled to weight the most high-variance parameters, so it can be presented and programmed in a more organized fashion while still representing the impacts of all features."
      ],
      "metadata": {
        "id": "GUECdhIKXJ10"
      }
    }
  ]
}